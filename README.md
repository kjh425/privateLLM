ë¡œì»¬ LLM + ì½”ë“œ RAG ì„¸íŒ… (Windows / íì‡„ë§)

ì´ì§í•œ íšŒì‚¬ê°€ ë‚´ë¶€ë§ì„ ì“°ëŠ”ë° chatGPTë¥¼ ë‹¤ë¥¸ ì»´í“¨í„°ì—ì„œ ì“°ê³  ë³µë¶™í•˜ëŠ” ê²Œ ê·€ì°®ì•„ì„œ ì§ì ‘ ì •ë¦¬í•¨.
USB â†’ íì‡„ë§ PCë¡œ ë³µì‚¬ í›„ ì•„ë˜ êµ¬ì¡°ë¡œ ì„¸íŒ….

-----------------------------------------ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

ëª¨ë¸ì´ë‘ ì„¤ì¹˜íŒŒì¼ì€ ì•„ë§ˆ ì‚¬ì´ì¦ˆê°€ ì»¤ì„œ ê¹ƒì— ëª»ì˜¬ë¦´ë“¯.
https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF?utm_source=chatgpt.com
ì— ë“¤ì–´ê°€ì„œ ë‚´ ì»´í“¨í„° ìŠ¤í™ì— ì—¬ìœ ìˆëŠ” ëª¨ë¸ë¡œ ë‹¤ìš´

íŒŒì´ì¬ ë‹¤ìš´ê²½ë¡œë„
https://www.python.org/downloads/release/python-3137/

ollama ë‹¤ìš´ê²½ë¡œ
https://ollama.com/download

```plaintext
C:\llm\
 â”œâ”€ models\
 â”‚   â”œâ”€ Meta-Llama-3.1-8B-Instruct-Q3_K_S.gguf   (ê°œì¸PCìš©)
 â”‚   â”œâ”€ Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf   (íšŒì‚¬PCìš©)
 â”‚   â”œâ”€ Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf   (ê³ ì‚¬ì–‘ìš©)
 â”‚   â”œâ”€ Modelfile-q3.txt
 â”‚   â”œâ”€ Modelfile-q5.txt
 â”‚   â”œâ”€ Modelfile-q3   â† í™•ì¥ì ì—†ëŠ” íŒŒì¼
 â”‚   â””â”€ Modelfile-q5   â† í™•ì¥ì ì—†ëŠ” íŒŒì¼
 â”‚
 â””â”€ tools\
     â”œâ”€ index_repo.py
     â”œâ”€ ask.py
     â””â”€ agent_apply.py    (ì„ íƒ: diff ìë™ ì ìš©ê¸°)
```

.pyë‚˜ í™•ì¥ì ì—†ëŠ” ìƒ˜í”Œì€ ë©”ëª¨ì¥ì—ì„œ â€œëª¨ë“  íŒŒì¼â€ ì„ íƒ â†’ í™•ì¥ì ë¶™ì—¬ ì €ì¥í•´ì„œ tools/ì— ë„£ìœ¼ë©´ ë¨.

-----------------------------------------âš™ï¸ ì„¸íŒ…
Modelfile ì˜ˆì‹œ

C:\llm\models\Modelfile-q3.txt

FROM C:\llm\models\Meta-Llama-3.1-8B-Instruct-Q3_K_S.gguf
PARAMETER temperature 0.2
PARAMETER num_ctx 2048
PARAMETER num_gpu_layers 0


â†’ ì €ì¥ ì‹œ í™•ì¥ì ì œê±° â†’ Modelfile-q3

C:\llm\models\Modelfile-q5.txt

FROM C:\llm\models\Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf
PARAMETER temperature 0.2
PARAMETER num_ctx 4096
PARAMETER num_gpu_layers 0


â†’ ì €ì¥ ì‹œ í™•ì¥ì ì œê±° â†’ Modelfile-q5

ì„¤ì¹˜ ìˆœì„œ

Ollama ì„¤ì¹˜

OllamaSetup.exe ì‹¤í–‰ (ê¸°ë³¸ê°’ ì„¤ì¹˜)

Python ì„¤ì¹˜

python-3.13.x-amd64.exe ì‹¤í–‰

ë°˜ë“œì‹œ "Add python.exe to PATH" ì²´í¬ í›„ ì„¤ì¹˜

ì„¤ì¹˜ í™•ì¸

ollama -v
python --version

-----------------------------------------ğŸ ëª¨ë¸ ë“±ë¡
ollama create <ì´ë¦„> -f <ëª¨ë¸íŒŒì¼ê²½ë¡œ>


ê°œì¸PC (RAM 7~8GB):

ollama create my-llama31 -f C:\llm\models\Modelfile-q3


íšŒì‚¬PC (RAM 16~32GB):

ollama create my-llama31-q5 -f C:\llm\models\Modelfile-q5

ğŸ—‘ ëª¨ë¸ ì‚­ì œ
ollama list


ì¶œë ¥ ì˜ˆì‹œ:

NAME             ID              SIZE    MODIFIED
my-llama31       3d82f19fâ€¦       3.4 GB  2025-09-06 11:20:00
my-llama31-q5    b91a01c2â€¦       5.7 GB  2025-09-06 09:45:00


íŠ¹ì • ëª¨ë¸ ì‚­ì œ:

ollama rm my-llama31

-----------------------------------------ğŸš€ ì‹¤í–‰ ë°©ë²•
1) ê·¸ëƒ¥ ì‹¤í–‰ (ë¹ ë¦„)
ollama run my-llama31

2) í”„ë¡œì íŠ¸ ê¸°ë°˜ RAG ì‹¤í–‰ (ëŠë¦¼)

ì„œë²„ ì‹¤í–‰

ollama serve


ì½”ë“œ ì¸ë±ì‹±

cd /d C:\llm\tools
python index_repo.py --root "C:\ê²½ë¡œ\ë‚´í”„ë¡œì íŠ¸"


â†’ tools/code_index.json ìƒì„±

ì§ˆë¬¸í•˜ê¸°

python ask.py --q "ì•ˆë…•? ë‚´ í”„ë¡œì íŠ¸ì˜ @@@ì— ëŒ€í•´ ì•Œë ¤ì¤˜" \
  --api "http://127.0.0.1:11434/v1/chat/completions" \
  --model "my-llama31"

-----------------------------------------ğŸ›  íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

ì˜¤ë¥˜:

Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address...
â†’ ì´ë¯¸ ì‹¤í–‰ ì¤‘. í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ í›„ ollama serve ë‹¤ì‹œ ì‹œì‘.

-----------------------------------------ğŸ“ ê¸°ë¡ì‚¬í•­

ëª¨ë¸ íŠ¹ì„± ë°”ê¾¸ë ¤ë©´ ask.txtë‚˜ index_repo.txt ì°¸ê³  â†’ ìˆ˜ì • í›„ .pyë¡œ ë³€í™˜ â†’ tools/ì— ë„£ê³  ì‹¤í–‰

(2025/09/06) Q3 ëª¨ë¸ ê¸°ì¤€ ë©”ëª¨ë¦¬ 90% ì‚¬ìš©. ê·¸ë˜ë„ ì½”ë“œ ì½ê³  ë‹µë³€ ì£¼ëŠ” ê±° ê½¤ ê´œì°®ìŒ ğŸ˜†