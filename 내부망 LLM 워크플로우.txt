1. 
https://github.com/ggml-org/llama.cpp/releases?utm_source=chatgpt.com
ì— ë“¤ì–´ê°€ì„œ
llama-b6394-bin-win-cpu-x64.zip ì²´ì œë¥¼ ë‹¤ìš´ë°›ìŒ.

2. ì¸í„°ë„·ì´ ì•ˆë˜ëŠ” íì‡„ë§ í™˜ê²½ìœ¼ë¡œ zipíŒŒì¼ ì˜®ê¹€.

3. https://www.python.org/downloads/release/python-3137/ 
ë§í¬ ë“¤ì–´ê°€ì„œ 64ë¹„íŠ¸ (ìš´ì˜ì²´ì œ ë§ê²Œ) ë‹¤ìš´ë¡œë“œ í•˜ê³  ì„¤ì¹˜íŒŒì¼ ì‹¤í–‰ì—ì„œ ê¼­ Add python.exe to PATH ì˜µì…˜ ì„ íƒ

4. í´ë”êµ¬ì¡°ë§ê²Œ ìˆ˜ì •

5. ollamaì„¤ì¹˜ ë° ë²„ì „í™•ì¸

ollama -v
ollama version is 0.11.10

(ollama ëª¨ë¸ë“±ë¡)
ollama create my-llama31 -f C:\llm\models\Modelfile.txt

(ì‹¤í–‰)
ollama run my-llama31

(í”„ë¡œì íŠ¸ ë¶„ì„)

cd /d C:\llm\tools
python index_repo.py --root "<í”Œì ê²½ë¡œ>"

í•˜ê²Œë˜ë©´ tools/ í•˜ìœ„ì— code_index.json ì¦‰ ë‚´ í”„ë¡œì íŠ¸ë¥¼ ë¶„ì„í•œ jsoníŒŒì¼ì´ ìƒì„±ëœë‹¤

ì˜ˆì‹œ)
cd /d C:\llm\tools
python index_repo.py --root "C:\Users\Admin\Documents\workspace-sts-3.9.17.RELEASE\jhproject"



(API)
curl http://127.0.0.1:11434/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\": \"my-llama31\", \"messages\":[{\"role\":\"user\",\"content\":\"í…ŒìŠ¤íŠ¸\"}]}"

python ask.py --q "JwtAuthenticationFilter ë¬¸ì œì™€ í•´ê²° ë°©ë²•" ^
  --api "http://127.0.0.1:11434/v1/chat/completions" ^
  --model "my-llama31"

(íŒŒì›Œì‰˜ì—ì„  tools í´ë”ì•ˆì—ì„œ...)
python ask.py --q '<ì§ˆë¬¸>' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"

python ask.py --q 'í˜„ì¬ ë‚´ í”„ë¡œì íŠ¸ì—ì„œ í†°ìº£ìœ¼ë¡œ 8080ìœ¼ë¡œ ì˜¬ë¦¬ë©´ ë‚˜ëŠ” hello ë¼ê³  ì›¹ì— ì°ê³ ì‹¶ì–´.
ê·¸ëŸ¼ api returnì„ í•˜ë“  jspë¥¼ ë§Œë“¤ë˜ í•´ì•¼ê² ì§€? ì–´ë–»ê²Œ ìˆ˜ì •í• ìˆ˜ìˆì§€?' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"

python ask.py --q '<>' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"

python ask.py --q 'ê·¸ëŸ¼ í˜„ì¬ homeControllerë³´ì—¬? ê·¸ ì•ˆì— /hello ë¼ê³  ë˜ì–´ìˆëŠ”ë° ë„ˆê°€ ë³¼ìˆ˜ìˆëŠ”ì§€ ëª¨ë¥´ê² ë„¤ ' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"

python ask.py --q '<>' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"
python ask.py --q '<>' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"
python ask.py --q '<>' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"
python ask.py --q '<>' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"
python ask.py --q '<>' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"
python ask.py --q 'ì•ˆë…•? ì¸ì‚¬í…ŒìŠ¤íŠ¸í•˜ëŠ”ê±°ì•¼' --api "http://127.0.0.1:11434/v1/chat/completions" --model "my-llama31"


ë©”ëª¨ì¥ì—ì„œ â€œë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°â€

ë©”ëª¨ì¥ì—ì„œ ë‚´ìš© ì‘ì„±

ì €ì¥í•  ë•Œ íŒŒì¼ ì´ë¦„ì— "Modelfile" (í°ë”°ì˜´í‘œ í¬í•¨) ì…ë ¥

í˜•ì‹ì€ â€œëª¨ë“  íŒŒì¼(.)â€ë¡œ ì„ íƒ
ğŸ‘‰ ì´ë ‡ê²Œ í•˜ë©´ í™•ì¥ìê°€ ì•ˆ ë¶™ê³  ì •í™•íˆ Modelfileë¡œ ì €ì¥ë¨

C:\llm\models\Modelfile ë‚´ìš©(ì‚¬ìš©í•  ëª¨ë¸ë¡œ êµì²´):

FROM C:\llm\models\Meta-Llama-3.1-8B-Instruct-Q3_K_S.gguf
PARAMETER temperature 0.2